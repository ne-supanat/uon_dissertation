IV8	This building houses about 100 staff, it’s relatively open plan.
S	How long have you been here?
IV8	We took possession in August last year, August [removed], moved staff in, in August [removed] and then moved computers into the computer room in the new year of [removed], early in [removed]. So when they design buildings, and I’m an IT man as a starter, I lived down here while it was being designed and built, so I know a fair bit about it. So apparently the way you design buildings, in office spaces, is you define a Watts per square metre, and that then allows you to work out roughly how much cooling you need, either natural or artificial. 
S	what produces that wattage? Do you know?
IV8	People. IT equipment, lights, pretty much anything that uses power also as a by-product produces heat. So I’m trying to think in here the watts per square metre. So this one is higher slightly than normal, because it’s an IT intensive environment. If you imagine the average desktop PC probably kicks out around 60 or 70 watts, your average monitor is about 20, 30. But you’ll find that a lot of people in here will not just have one on one PC monitor, they’ll have 2 PCs, 2 monitors, sometimes 2 PCs and 3 monitors. Just because in an IT function you’re going to use slightly more IT. So the watts per square metre is slightly higher than in a normal office environment.
S	So there’s a need to – what type of kit have you got, by the way, in terms of air conditioning, or cooling?
IV8	This building is mixed mode in terms of things. So we’re sat, just to give you a bit of guidance, we’re sat approximately there [referring to diagram]. So you can see left and right, this is all office space, then in the core -  in the big block – these are windowless server rooms. Then you’ve got some more office/meeting space over on the far side. So the idea is that machine rooms are all mechanically ventilated in their entirety. This office space here is mixed. So for the vast majority it is naturally ventilated, as far as possible, and then only when external temperatures are outside parameters does it switch to mechanical ventilation.
S	So what are those parameters?
IV8	External temperature is between something like 8 and 18. Because obviously if the external temperature is sub 8, you don’t want to be bringing 8 degree air into the building, it’s too extreme. But as long as it’s above that then we can start using that cool air.
S	I see, so as soon as the external temperature rises above 18 degrees centigrade, it starts to mechanically cool the air?
IV8	Yes. And then it does that, and in this space as well we’ve got a lightwell air manager, and then we have high level opening vents which are on the Building Management System. So when it needs to use a natural vent, it will open those to an appropriate angle, and then either people can open their own windows, draw through and out. 
S	OK. Is it attempting to keep the indoor air at any particular...
IV8	Indoor I think is set for 21-25.
S	OK so there’s a couple of degrees variation there?
IV8	Yes
S	So it can move between those temperatures ?
IV8	Yes. 
S 	What happens when it gets to 25?
IV8	Internal
S	Yes	
IV8	Obviously if it gets to 25 internal and the external is 18 then it’ll just have everything open and it’ll assume that people just don’t have their windows open, they’re comfortable. Obviously, when it’s in a mechanical mode it will blow.
S	OK, so it only goes to mechanical mode once the outdoor air temperature reaches 18 degrees, and that then starts to blow air in?
IV8	Yes correct. And if you look down there then you’ll see there are boxes, traffic lights on the wall, and you see the blue light all the way down the bottom – I’ll show you on the way out – they let the building users know what mode the building is in. So the blue light means we’re in a mechanical vent, because it’s colder than the air outside. But then at some point, probably in about half an hour, you’ll find that will flick to green, when it’s natural. 
S	So when it’s in natural, are the windows automatically opening and closing, or do people have to do it themselves?
IV8	They open automatically. The ones next to people’s desks, the users obviously have control of that, because some people won’t want open windows, they’re quite happy, so you’ve got this mix of centralised control to give the volume and individual control so that each desk position, as far as we can do, has an element of control. 
S	That’s really clever. What temperature are the server rooms kept at in the core?
IV8	Server rooms are kept at 22 at the moment. 
S	And where does 22 degrees come from?
IV8	22 comes from, traditionally that is slightly up on – traditional server rooms, going back a couple of years, were set at 18. That’s what you’d have. You’d have server rooms at 18, end of story. That’s been increasing over time. And depending on whose thing you take, it is going up as high as 30. So I don’t know if you’ve come across an organisation known as ASHRAE?
S	Yes
	
IV8	ASHRAE have their chart with the humidity and the overlay, and their pushing and they’re even going up to 40. You need to temper that a bit. I think ASHRAE have gone a bit – personal view – but I think ASHRAE have gone a bit too far towards grabbing the headlines rather than looking at serviceability, anecdotal evidence for instance, it’s not always just a straight ‘have a higher temperature and save more money’, the – at [removed], they tendered for a new high performance cluster, and they sent off all their parameters in these tenders, and they said this is all the power that you can draw in each rack, and these are the parameters of our server rooms. And DELL won that bid, and loaded up the racks as per... but DELL had missed an important point, in that DELL’s standard calculation for power assumed a server room running at 18 degrees, [removed] ran theirs at 22. That 4 degree caused the fans in the DELL equipment to run at a higher speed, which drew enough extra power to trip the breakers in the rack, so then what DELL had to then is start spreading the servers out further, so then they had gaps in the racks so that the overall power draw was lower. So it’s not – I can’t put any more quantifiable measurements on than that, but you have to balance it, which is why we’re at 22. And probably over time you’ll pick it up towards the mid 20s, but you also have to think about the pleasantness of the environment, that when you’re in there, you don’t want to sit in your shorts – it’s not pretty – so there’s that as well. And the other thing is, it’s not just the input temperature, it’s the output temperature as well. Delta- T across the equipment. Because if you imagine server equipment draw cold air in across the front, pass over as many components as it can to cool them, and then expels hot air at the back. And as IT equipment gets more and more dense, we’ve gone from servers that were about this big, that drew 200 watts, to servers that are a pizza box, an inch high, that are drawing 400 watts. And now we’re getting blade centres that are about a foot tall that are drawing several kilowatts. So if you imagine if we’re putting 22 degree air at the front, we’re seeing typically 30-35 degree air at the back, and if you put that up to 30 degree air at the front, you’re talking about 50 degree air at the back, which is starting to become a risk for hot metal and general safety, and the jacketing on cables, and there’s more things to think about. I expect we’ll see that will increase.
S	You were saying that DELL for example, have designed their server to operate most efficiently at 18 degrees centigrade.
IV8	That was their parameters, I wouldn’t say most efficiently, but I’d say that was their standard design method, yes.
S	How does that work. Do the temperatures that these pieces of equipment are designed to run at, is that in some way designed into them, or is it a fundamental property of them? 
IV8	I think what you’ll get is a typical operator – so if you go to a datasheet for any piece of equipment that’s designed to go anywhere – even just a PC, it’ll have an operating temperature. And that operating temperature typically is 10-40 degrees with a non-condensing humidity factor as well. And like I say, ASHRAE are pushing that – are pushing manufacturers to push that out. But obviously the air flow has to improve as the input temperature gets hotter. But whilst the chips on the board are quite happy to run at higher temperature, you need to be pushing more heat, more air across them. Because obviously, if you have a disparity of 10 degrees, then it’s going to work a lot harder than if you’ve got a disparity of 20 degrees. It’s just about the performance of air as a cooling mechanism, it’s not necessarily the most efficient mechanism of heat transfer. And then you’ve got other components that may not be suitable, so we’re already seeing some data centres where they’re hiving off some of the more sensitive equipment – like tape-drives for instance – into areas with different environmental parameters, so they can run the bulk of their equipment at a higher temperature, but their tape drives are still running at 20 degrees, because obviously if you think about, we’re talking about 30 degrees for input temperature. If you look at on-surface temperature inside the chassis it’s going to be much higher, and if you look at most tape drives you’ve got an optimum tape temperature of 30 -40.  So your equipment, your motors, your belt drives are quite happy, but your tape onto the tape head, potentially you want to be cooling. So you’re starting to see the split in environmental forms within rooms. 
S	And as operating temperatures- the envelope – becomes wider, and the normal operating temperature of a piece of equipment moves up, what sort of challenges – you’ve gone through a few – do you think will be  - what sort of challenges will there be in terms of designing the kit to run? Do you think it’s possible to design the kit in that way? 
IV8	I think it is, but what you’re seeing now, if you look at the average front of server, they’re cutting down remarkably on the amount of things on the front of it, and it’s becoming a giant air grate, because the challenge is to get so much air through. And it’s always a problem that real estate is expensive, so you want your servers to be as physically small as possible, but then they need to get as much air through as possible. And if you open now a [removed] wrap round server it’s like an F1 car, but with plastic rather than carbon fibre, in terms of the cowling and the ducting, to force the air to go in a particular manner, because it’s a challenge to get the air flow through it. Because the front of a DELL server behind the drives is literally a row of 6 fans from left to right.  
S	What do you, do you think there’s a limit to the operating temperature of these pieces of kit, in the sense that could they keep pushing up the air intake temperature? IF they were redesigning the kit..
IV8	I don’t know, I’m not a silicon man, but you know the two major components that drive heat, are your hard discs which are magnetic platters, and INTEL or OMD who make your processers, they’re the two big – it’s your silicon and your discs. Now you can take, for low power usage, and this is something we’ve seen google do – they’ve gone to solid state – now that’s very expensive for mass data storage but it’s not too bad for operating system storage. And they output less heat than a traditional spinning hard disc and you have slightly higher operating parameters, so then your back down to your chips and you’ve got to look at INTEL and OMD to – where do they see their operating temperature go – and it’s driven by two factors, because you’ve got your on temperatures, but you’ve also got your core temperatures as well, so quite often you’ll see a core temperature...
S	What do you mean by core temperature? 
IV8	If you look at a chip, internal they’ve then got cores, generally you expose the core temperatures through monitoring tools, it’s actually the physical surface temperature of the chip will drive an even higher core temperature, it’s actually the core temperature that you’re worried about because that’s the point at which it will start degrading, and actually damaging itself. So depending on how efficient air flow you can get across it and how efficient your heat sink is, which is why you’re starting to see for some of your bigger stuff, you’re starting to see the return to the old mainframe type technologies of liquid cooling direct onto the chip.  Which I remember when I did my work experience back in 94, when I went to a council and they had a big mainframe with water flowing through it, and it’s going back – it went away from it – and it’s going back to that sort of on-chip water cooling.
S	So that’s really interesting, and you were saying in the lift as well, about how technology has evolved. Maybe you could say a little something about that. So how now do you see the kit that you’ve got increasing – I mean do you see it increasing in energy usage into the future, or in amount, and how has it got to this point?
IV8	Pause... So that’s 1986, that’s our machine room in 1986. [referring to presentation picture]
S	Crikey, is that here in [removed]? 
IV8	Yes. Tape drives, hard discs and that’s your computer. It actually had wooden panelling on it like an old telly, it really did.
S	Wow, ok and what sort of – in 1986 were there operating temperatures that needed to...
IV8	Yes. This was the old 18 degrees. You can see people are in there, they’re wearing coats , they’re wearing jumpers. It was a largely occupied space, you know these things needed a lot of love and tender care – there were people, you know this was their office, and they’re in a jumper, this is your 18 degree days. Not a lot of thought into air circulation – you can see that’s got wooden panelling on the front. It’s not grilled at all. 
S	So they would have to keep the whole room at 18 degrees?
IV8	Yes, that’s pretty much the way it was done. There wasn’t – you know 86 – you did it because that was the way that people did it. Fast forward to 2003 and this was the proliferation of change. So if we just take on the left screen, this was probably late 90s/ early 2000s where we’re moving away from the big computer through to the tower type system. Again, we were talking about this, you can see a lot of grills on the front, but they’re still fairly chunky pieces of kit – this is fairly large. They’re still bigger than your average desktop PC. You’ve got a lot of air flow on the front. You fast forward to the 2003 vintage equipment, these are 1u and 2u wrap round servers, so you’re taking all of the power output and heat output of that, and putting it into a much smaller box. And because this was our old machinery it wasn’t purpose built – it was purpose built for the old style and not the new, so this was lots of ceiling mount air conditioning units using refrigerant technologies, and...
S	That’s really interesting, so this room was built to cool 1990s technologies.
IV8	Well no. This room was built to cool that. It was the same room, same single space. So this building was put up in the 70s, that’s the earliest picture that I could find and we then re-purposed it to store that on the racks, so these massive things of shelving with loads of those on...
S	And was it still being kept at 18?
IV8	No, this was just by density, this was about 21. But increasingly more, because if you imagine that if you’ve got traditional ceiling mount cassettes, it’s difficult when you’ve got a ceiling space to coordinate them. So you’d have one blowing into the other and knocking each other off. So increasingly we, you know towards the end it was a miracle rather than planning that it was still running. And the other thing is that they were office units, so they weren’t really ready for continuous duty, because one of the things about server rooms is that the heat load doesn’t go away. An office space the heat load goes away. It goes away when people go home. It goes away in the winter. In a server room the load is fairly static. Certainly not that you’d notice. If you drill down then you’d see that outside of core hours things acquiesce slightly – people aren’t using the filestore as much, so the discs don’t spin as much, but it’s a percentile – it’s small. 
S	So you’ve still got the same cooling kit here in the 1990s as you had in the 1980s, but the density of the equipment has increased, so as a result, even though the same cooling energy has gone into it the temperatures gone up.
IV8	Yes. And then this is a different room, but again you can see lots of servers on racks, you can see the traditional ceiling mount cassettes there, and trying to get the airflow through. And this is – now knowing what we know, we would never have installed this in this way. Because you’ve got the rear of the equipment, front of equipment. So you start the cool air there, it gets hotter  - ooo look, it gets sucked into there and gets hotter still. So now you know, you’d never put them in that orientation, but these were put in, in 99-2000.
S	Ok, so just quickly going to this one here, this is the same room still? 
IV8	This is – that room, that room and that room are the same.
S	OK, so this is the 2003, and it’s still in the same room. Is it with the same cooling equipment?
IV8	Not exactly the same, because obviously stuff you put in 1986 won’t still be spinning in 2003, but pretty much in terms of kilowatts and technology. 
S	And so what temperature was the room kept at in 2003.
IV8	We were probably aiming for 22. But reaching 24, because you couldn’t achieve that.
S	OK, and then in the new room...
IV8	That’s not a new room, that’s an old room – that’s probably a 2000 vintage.
S	OK, ok. 
IV8	Targeting 22
S	Targeting 22. So how close – so you don’t always get to 22? 
IV8	No sometimes not actually because of air flow challenges. Because one of the things that makes the cooling more efficient is to manage your air flow, and there was no air flow management. Fast forwarding to today, I’ve got some gratuitous fancy shots, we’ve got two different cooling techniques in this building, and that’s on density, trying to match the cooling that we’re using to the demand. So this here, it’s not a very big picture, is what you’d call traditional computer room air conditioning. So you have a raised floor void, you have air conditioning units in the room, that blow cold air into the floor void. Then by the virtue of placing grilled tiles the cold air is delivered to the front of the racks, sucked through the racks and then returned to the air conditioning unit. So you manage exactly where you want your air flow to go, in that it only goes to the front of the racks, not anywhere else, and it’s a closed loop system. So you can see here – this is a slightly different angle – so there’s the racks, the bottom is your air conditioning units. So you can see here, this is before it was finished. So the black unit blows air into the floor, the air comes up through these grilled tiles here, nowhere else in the room, and then the only place it can go is through the servers and then out the other side. We make it slightly more efficient, you see that there’s doors at the end there, and then there’s a lid on top, and now when we walk into the room in a bit, there’s doors on this end as well. So that means that there’s no ability for the cold air to go anywhere other than the place it needs to cool. So trying to minimise wastage. Because otherwise you can imagine you’re just blasting it into this room and hoping for the best, you’re going to not be as efficient as you can. So by containing it – this is called cold aisle containment – we’re trying to make sure it only goes where it’s needed. The hot air then goes through the servers, out the back, where it returns into the top of the unit and back for instant repeat. This type of cooling tops out in terms of maximum efficiency, capacity and capability, at around 5-6 kilowatts per cabinet. If you go beyond that you have to increase the level of the floor void quite substantially. So this is a 500 mm floor void, and if you want to go beyond that – I’ve seen examples where they’ve gone 8-10 kilowatts, but they’re building a 1.2m floor void, and you have to run the fans at a much greater velocity because you just have to have that volume of air, so we decided in conjunction with the electrical engineers and mechanical engineers on this project that the realistic operating capacity was 5-6 kilowatts. Now that works fine for some categories of equipment, but there are others like these blade centres we’re doing server virtualisation on, we need much higher than 5-6 kilowatts per rack, and for our higher end computing we need much higher again. So what we did, we went for direct water cooling, and what this is  - you’ll see there’s no big air conditioning units at the bottom of each row, there’s no grilled tiles, and instead what that’s doing, it’s taking the equivalent of one of those air conditioning units and strapping it to the side of the rack. So what that does, it’s got a heat exchanger in it, and it blows cold air in the front, directly in front of the servers, so the servers sucking it through, where it pulls it back into the rack into the background again. These here, each individual unit can go up to about 30 kilowatts, and then obviously by sharing it between two racks we can do 15 kilowatts a rack quite easily. And this is what we’re doing with the higher density stuff. And again, the fans can run a lot slower, and generally it’s a neater way of doing it and getting that high density, because otherwise you’d end up – in this scenario – you might have a massive rack and you might only be able to put one piece of equipment in it, because that’s the density, but this one we can fill it up. Because efficiency generally in terms of carbon, is the cost of building it as well, and if you have to build far many more square metres than you need just to sparsely populate it, then that’s wasteful in its own right, so we try and maximise. But then it would be overkill to try and put everything in these style of cabinets. So we’ve got different rooms with different densities.
S	So are these expensive to run in terms of their energy consumption?
IV8	 No. Not anymore. You know they’re about the same because they’re a smaller version, you know they’ve got fans in the back that are variable speed. They’ve got valves in the bottom that only take the amount of chill – you know they oscillate between 0 and 100 % to say I only need 7% chilled water to do this, so you’re not, you’re trying to minimise how much they take in terms of electricity input and water input and then that minimises the load on the chiller circuits as well.
S	It looks like, from this photograph, that this is a contained system, so it’s only actually cooling this area here as opposed to the whole room.
IV8	Yes. So we keep the room a bit warmer.  The room does have – and I think this is a mistake on the behalf of our consultants – because I don’t think there’s any purpose for these units you can see here...
S	The air conditioning
IV8	 Yes. They’re comfort cooling. It was thought at the start that because you’ve got warm air here that they will heat up the rear door and that will then act as a radiator into the rest of the room. So they put – it’s fairly low level in terms of kilowatts, and they’re not massive units, but they put comfort cooling in just to make sure that it didn’t act as a radiator. I suspect that that was then arse covering, than an actual needed design decision. But obviously there’s a point at which we have to rely on them to do that for us.
S	Absolutely. That’s really interesting. 
IV8	That’s over the road, so these are the two chillers that provide – so the other way that we’re trying to be as efficient as possible is obviously the bigger the units you buy, the more efficient generally, they are. So  we’re using the same chilled water to cool the offices and the machine rooms. So it’s predominantly for the machine rooms, but we’re not running little units, we’re running big chillers that we would be running anyway, so the incremental on them is minimal. I’m trying to show you some of the other... so essentially every watt that we put into a server we have to expel somewhere, and then actually the IT equipment is – this is quite an old slide and this has changed slightly now, but the IT equipment is a relatively small percentage of the overall power draw. So if you look at a lot of the IT things that we do to make things more efficient, like the virtualisation of servers, so for instance we’ve got a blade centre that’s about yey big and drawing 3 to 4 kilowatts, but on it there’s a hundred servers, which are probably worth 20 kilowatts. You know, by doing the virtualisation technologies we’ve minimised the IT elements of it. Obviously there’s a knock on impact, but we’ve got these other losses to accommodate with too. So things like switch gear/ generators are largely American, an Americanism, this slide is a PC from an American company. Americans –  110 volts isn’t very easy to distribute, they’ll talk an awful lot about PDU losses and transmission losses, we don’t really see that as much in 240. So things like lighting – all of the server rooms have PIR lighting – so lights off normally. UPS’s you have to try and get the most efficient UPS that you can. They’re talking about 18%, our 
S	That’s the power system
IV8	So if the power fails, they run on batteries briefly. Now, they talk about 18%. Ours is about 12% efficient, and this again was a by product of how we built buildings, and generally of how buildings are built, in that you put out a performance specification and then the contractor tries to buy the cheapest they can to meet that performance specification. So we didn’t get what I would call a best in class UPS. We’re about 12%. Best in class now is about 8% efficient. I mean loss, not 8% efficient – that would be terrible – 92% efficient, no 8% loss. The units – obviously you’re using fans and fans take power, humidifiers – you need to control humidity in machine rooms – air conditioning is a very drying technology , so you try and minimise it. So we’re trying to minimise it because we’re using water rather than refrigerant. So the water is running at about 11 degrees which is above dew point, which means it doesn’t have as much of the dehumidification as refrigerant, which is probably at 2 or 3 degrees, but they still have steam [removed] just in case. Don’t want to dry the air out too much. And then the chillers, by virtue that you’re running – and this is the metrics that I was trying to write in the earlier email exchange about how much power you would spend on IT – it goes elsewhere, and there’s some technologies to try and reduce that now. So you’ve probably come across the PUE figure, have you?
S	Yes, vaguely, yes.
IV8	Vaguely, so PUE is you take the total data centre power divided by the power consumed by the servers and you get a ratio, and then good is between 1.2 and 2, your industry average is 3, and then you’re seeing state of the art at 1.2. Again you’ve got to be careful on some of the green-wash on some of these state of the art data centres, because one of the ways you get down to 1.2 is by eliminating resilience in your data centre. So when we walk into the data centre in a bit, you’ll notice that we’ve got 2 crack units, and you’ll see we’ve got to chillers over the road.
S	So, just quickly, what are crac units?
IV8	Sorry, computer room air conditioning. 
S	Ah, got you, yes. 
IV8	Yes, so there’s two, and obviously they’re running at 50% load, which is not as efficient as running one at 100%, but you need two there if one fails. So what you’re seeing now, is like Google and Microsoft are saying we have data centres with a PUE of 1.1, we’re great! But they don’t have double CRACs, they don’t have double chillers, they don’t have UPS’s in some cases, but they’ve got another data centre – a duplicate – on the other side of the country, or in another country. So in order to get that in this location, they’re running 2, which doesn’t mean that they’ve got, if you look across their data centre estate, they’re remarkably inefficient, but if you look this individual data centre. And that’s where the next lot of metrics from things like the green grid, are now saying well we need to look at not just an individual facility, but we need to look at it across your organisation, to make sure that you’re not able to put out that we have a data centre of 1.1, when actually we’re running two data centres flat out to make sure we’ve got that resilience number. And then some people didn’t like ratios so they invented DCIE which is essentially that turned into a percentage. This is the Green Grid, but there are a few different organisations that are trying to promote green data centres. There’s the green grid and then we’ve got – which is predominantly an American organisation. We’ve now got the British Computer Society launched in November this thing called CEDA, which is a data centre award which you can go for and try and track your green credentials. And then we’ve got the UK code of conduct for data centres which you might have come across as well. That’s a fairly decent document in terms of data centre design principles and things you can do, and things that they would look for you to do. They would just look for you to comply with that, and that’s something that I want us to do, because I think that we would largely comply with it. This is more of a commercial – you pay to be assessed - and get gold, silver, bronze awards. Largely if you’re bidding for third party business, you want to be able to certify that the company putting stuff into your data centre is doing it in the right way. That’s why the ability of something like Computer Centre – that’s a massive outsourcing provider – is able to say you’re not just outsourcing your services to us and we’re great, we’re also a really efficient gold star data centre. So that’s that. This is free cooling, which you may have come across.
S	I don’t think I have actually, or not here at least.
IV8	OK, so this is the traditional free cooling, this is the method that we use here. It’s moved on again from there since, and if we went to rebuild this data centre again, then we’d use better technologies again, so basically your chillers over there has two components to it. You’ve got a compressor, which is actually your chilling mechanism, and then you have fans, which is actually like a radiator in a car. What this says, it says if we look at the UK temperature, and we’re aiming for 10 degree water – so let’s just be very conservative with this and say when the air temperature is less than 5, we can take that and just use it like a car radiator. There’s no reason to run the compressors, and if we just blow air through it – through a car radiator – it will drop that water down to 10 degrees. And if we look at how often the temperature is below 5 degrees – it’s 25 % of the year. So for 25% of the year you use it like a car radiator and don’t run the compressors, so you’re saving a lot of money. So that’s what we have over there, we have 3 cooling chillers. The other thing that they’ve got in, is they’ve got a multi-stage compressor. So normally the 800 kilowatt chillers, the compressors on them probably draw 100 kilowatts at full load. So we’re running this all the time, and if you’ve got a single stage compressor then it’s either on or off, so it’s constantly cycling on and off, so it’s doing maximum demand surges, but also it means that you’re running 100 kilowatts of compressor, when actually the outside temperature might only be 10. So it’s not cold enough to do free cooling, but you might get some free cooling, and so those chillers over there have an 8-stage compressor, which means you’re turning it on in 12.5% increments. So it’s trying to match the demand to the energy draw. So that’s what we’ve got over there. 
S	Fantastic, in that sense thinking about the way that the data centre was designed and built, you were saying that it was built and designed by [removed], and so what guidance did you give them as to what you wanted, in terms of the air conditioning, and what was it – or was it more that they came back to you and said this is what you’re going to need. 
IV8	Yes, because we, what we tried to do, we started designing this building in 2005, so a lot of the drive for green was only just starting. If you were doing it now, you’d say I want a data centre that has a PUE of 1.5, for instance. Whereas in 2005 we  - I didn’t  - I mean we talk about it now a lot  - but in 2005 we were talking about networking, we were talking about chip cooling. So we said we’re the IT guys, you’re the experts, we need this many racks, and we know that we’re going to put this much equipment in it, so we know that it’s going to need to use about this much energy. Because they were the only parameters that we knew, and then said to [removed] design us a building. So all of the air conditioning technologies, like the free cooling and the set points came from (company name). Knowing now, we’d probably have said, we need to draw this many  kilowatts and we want to ...So (company name) looked at several things about energy saving, but it was more about cost. So for instance they said that those chillers over there were 20% more expensive than their non-free cooling equipment. But given the energy drop, that percentage difference pays for itself in about 18 months. So it was done on that basis. There wasn’t a similar argument for UPS and I think we should have made it – now knowing what we know. Especially – UPS is fairly lengthy – so when we come to re-procurement in 15 years, we’ll almost certainly be having a different conversation with the consultants we get to design it. We’ll be saying we want you to design this and we want you to allow for this many kilowatts. 
S	So this idea -  the greening of the IT industry, in your experience where did this start and how’s it progressing? Where is this coming from?
IV8	Personally, I – we started to see it in 05, but not a lot. It’s certainly accelerated over the last two years and it’s accelerating still, you know one of the main drivers is that everyone is becoming a bit more aware of ecological issues, and then you’ve got things like the carbon reduction commitments and the carbon tax, and that’s waking people up, it’s waking up the likes of the Chief Financial Officers of corporates, and the need for sustainable procurement. There’s all these sorts of things that are adding up to a perfect storm in terms of awakening people to being green. And the utilities costs as well. It’s not just about the greenness. If something takes more power in terms of input then it’s going to cost more to run. The [removed] utilities bill is in the region of £5million a year, if you imagine, if you just make a 5% saving off that, that’s several people employed. It’s a significant percentage of the [removed] turnover. 
S	So you’ve mentioned the carbon tax. Are there any other regulations that you have to bear in mind when you’re running a data centre like this? 
IV8	Not that I see, because I think that those sorts of carbon commitments get bundled into the [removed] Estate as whole, and are managed out of facilities rather than out of an IT element, so it will be someone like [removed] that you speak to on that. 
S	And that’s – you’re interaction with the [removed] - to what extent were they involved in the planning and design of this building? 
IV8	Every building is built by Facilities, it’s not built by an individual department so the whole project management, the design, the appointment of (company name), the sign off, was all managed by Facilities with us as the end user. So if you imagine in terms of architectural processes, Facilities were the ones, not us. We were the end user, but in terms of signing off that was managed by Facilities in it’s entirety.
S	OK, so they were the people that gave (company name) the spec.
IV8	Yes
S	And what, just quickly, can you remember what that involved, in terms of air conditioning. Was it just the power usage, or was there an attempt by them to limit the air conditioning or anything like that?
IV8	Well at the end of the day, the IT strategy drives the amount of power that we have in the machine rooms. If you want 1 gigabyte email boxes then we have to have that amount of IT equipment, so the parameters really was the amount of equipment that we knew we needed to deliver the amount of IT support. So that was not questioned. What was questioned was do you need 18, no you don’t we’ll go for 22. And then this is the amount of cooling to support that level of IT equipment. But the driver of IT equipment is generally by the IT strategy and the practice is that people want more and more.
S	So what is the IT strategy and how’s it progressing, in terms of the [removed]?
IV8	We have an IT strategy, it’s on the website, it’s in the process of being refreshed, but essentially it will say things like, we will have a robust communications platform, you know we know that people at the moment – I don’t know if you’re on [removed] – but we have people saying all the time I’ve filled my quota on [removed], I need more quota on my [removed] drive, so our level of IT input is largely driven by our user base. If people want more than 1 gigabyte on their [removed] drive then we need to provide more discs. More discs mean more power and it drives on from that. 
S	And just briefly, coming onto the final thing – control of the temperature and control of the air conditioning, is that here in the building or is that controlled elsewhere in the university?
IV8	It’s all controlled by the BMS, the BMS is autonomous in the building – so the last thing you want is for an incident somewhere else on campus to affect – but essentially it’s all managed by the Building Management System over in Estates. 
S	And how is that in terms of interacting with the University – are there control issues there? Have you ever had any problems with that at all? 
IV8	Again, it’s all about relationships and I work very closely with the people that manage the BMS. And there’s things like were trying to maximise – the server rooms are fairly static – they go in at 22 and they just work, but then you get into more personal issues out in the office space, like the traffic lights to let people know what building they’re in and give them opening windows, and some of the thermostats also have temperature wheels on them, so you can’t control your temperature infinitely, but you can do plus or minus two degrees from set point, so the office space we target 23, and I think we take that to 25 or 21, through the use of the dial. So giving people a modicum of control without making it horrendously inefficient. 
S	And in terms of comfort cooling, is the office space where it’s mechanically cooled, around 21?
IV8	No about 23. I’d have to go back to the original documents, but it’s not 21 it’s around 23. Well it’s actually 21-25 depending on the thermostats – because they’ve got a comfort wheel. Because obviously this building got BRIAM excellent, and a lot of the things about BRIAM excellent is not only being energy efficient, but giving workers a pleasant working environment, so things like natural light, and that’s one of the reasons this width here is less than 6m, because that gave the maximum draw through in terms of natural ventilation, to maximise the air changes. Then in winter there’s actually hot pipes at the top to act as a heat stack, so even if it’s not natural it will just draw.
S	And does the temperature throughout the year in terms of the offices?
IV8	Internally?
S	Yes
IV8	 It’s not bad. It does vary, but it’s not horrendous. I mean if it’s summer you know it’s summer. And if it’s winter. But that’s just more you can feel the change of heat. So heating for instance is radiators, so you know that your heat comes from a different source. A day like today you’ve got a fair bit of solar gain coming in. They’re fairly good windows, but you still feel the sun physically on you. It’s not warming the air, but you feel different, so it’s things like that, so it’s probably more psychological than actual change – it would be interesting to see the plots off the BMS. There’s no doubt it does change. 
S	Thank you very much.
....Contd
IV8	What you would do now if you were doing the data centre, and there’s a real one of this just opened in the [removed], if we take the plot chart and we take some arbitrary lines, and we say that when the  temperature is below 20 degrees external, and the humidity is above 50 degrees, these are the plots of daily temperature for [removed], and it’s remarkably, predominantly in the right area – so sub 20 degrees. So only these very few shoulder areas where you’d have to take action. So there’s actually a data centre now that uses  fresh air – just outside air – not even chillers, not even water for the vast majority of it’s time. So they’ve fixed it into what was going to be a distribution warehouse – so if you imagine a supermarket distribution warehouse – this floor void here is 5 metres deep, these fans here are 2 metres in diameter – very, very slow turning so they’re energy efficient, and literally air just blows in, through some fairly complex filter arrangements, just blows in outside air into here where it goes up into the data centre through a similar grilled tile arrangement and then is extracted, and by that point, for the vast majority of the year they don’t use chillers at all. They’ve got chillers there because this is next to a main road, next to a motorway, and it is [removed] so its chemical plants, so they’ve taken the decision that either when the shoulder periods are in, or when there’s an incident, they can close these dampers and run mechanically. So capital costs, it’s a fairly intensive way to do it. In terms of green running costs it’s incredibly energy efficient. So that’s the next one and that’s proper free cooling, and up in [removed] – if you’re anywhere North of Birmingham you’ve got that ability, and that’s how you’d do it now. That wasn’t about in 2005.
S	And that’s actually up and running in the [removed].
IV8	[removed] which is a massive outsourcing – well it’s been bought by [removed] now, so [removed]. 
